apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: macro-cycle-hmm
  namespace: default
  annotations:
    serving.kserve.io/deploymentMode: Serverless
spec:
  predictor:
    sklearn:
      storageUri: "models:/macro-cycle-hmm/Production"
      resources:
        requests:
          cpu: "100m"
          memory: "512Mi"
        limits:
          cpu: "1000m"
          memory: "1Gi"
    minReplicas: 1
    maxReplicas: 3

