services:
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ../.prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
    networks:
      - monitoring
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=2014
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=http://localhost:3000
      - GF_INSTALL_PLUGINS=
    volumes:
      - grafana-data:/var/lib/grafana
      - ../.grafana/provisioning:/etc/grafana/provisioning
      - ../.grafana/dashboards:/var/lib/grafana/dashboards
    networks:
      - monitoring
    depends_on:
      - prometheus
    restart: unless-stopped

  # Optional: MLflow tracking server
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.8.1
    container_name: mlflow
    ports:
      - "55000:5000"
    environment:
      - MLFLOW_BACKEND_STORE_URI=sqlite:///mlflow.db
      - MLFLOW_DEFAULT_ARTIFACT_ROOT=/mlflow/artifacts
    volumes:
      - mlflow-data:/mlflow
    command: mlflow server --host 0.0.0.0 --port 5000
    networks:
      - monitoring
    restart: unless-stopped

  # Optional: Feast CLI / feature store
  feast:
    image: python:3.11-slim
    container_name: feast
    working_dir: /workspace
    volumes:
      - ../..:/workspace
    environment:
      - FEAST_REPO_PATH=/workspace/.ops/.feast/feast_repo
    command: >
      bash -c "pip install 'feast[local]>=0.36.0' && cd $$FEAST_REPO_PATH && feast apply && tail -f /dev/null"
    networks:
      - monitoring
    restart: unless-stopped

  # Airflow DEV (for dev/* branches)
  airflow-dev:
    image: apache/airflow:2.9.3
    container_name: airflow-dev
    ports:
      - "8082:8080"
    mem_limit: 1.5g
    mem_reservation: 512m
    cpus: 0.75
    environment:
      - AIRFLOW_HOME=/opt/airflow
      - AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
      # Use FAB auth manager so 'airflow users create' controls login
      - AIRFLOW__CORE__AUTH_MANAGER=airflow.auth.managers.fab.fab_auth_manager.FabAuthManager
      - AIRFLOW__WEBSERVER__RATELIMIT_STORAGE_URI=redis://redis:6379/1
      # Optimize for resource usage - use single worker to reduce memory
      - AIRFLOW__WEBSERVER__WORKERS=1
      - AIRFLOW__WEBSERVER__WORKER_REFRESH_INTERVAL=60
      - AIRFLOW__SCHEDULER__JOB_HEARTBEAT_SEC=10
      - AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC=10
      - AIRFLOW__WEBSERVER__WEB_SERVER_MASTER_TIMEOUT=180
      - AIRFLOW__WEBSERVER__WEB_SERVER_WORKER_TIMEOUT=120
      # Make host project package and custom operators available inside the container
      - PYTHONPATH=/opt/airflow/src:/opt/airflow/operators
      # Set environment for config.py (since git commands may not work in container)
      - AIRFLOW_ENV=dev
      - GIT_BRANCH=${GIT_BRANCH:-dev}
      # PostgreSQL connection (use container name, not localhost)
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_USER=dev.tradingAgent
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-tradingAgent}
      - POSTGRES_DB=postgres
    volumes:
      - ../.airflow/dags:/opt/airflow/dags
      - ../.airflow/logs-dev:/opt/airflow/logs
      - ../.airflow/plugins:/opt/airflow/plugins
      # Mount custom operators package
      - ../.airflow/operators:/opt/airflow/operators
      # Mount project source (contains the trading_agent package) into the container
      - ../../src:/opt/airflow/src
      # Mount requirements.txt so we can install dependencies
      - ../../requirements.txt:/opt/airflow/requirements.txt:ro
      - airflow-db-dev:/opt/airflow
    command: >
      bash -c "
        # Install critical project dependencies needed for DAGs (run in background, don't block)
        python3 -m pip install --quiet tqdm pandas psycopg2-binary requests python-dotenv > /tmp/pip_install.log 2>&1 &
        # Start Airflow services immediately (dependencies will install in background)
        airflow db migrate &&
        airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password 2014 || echo 'Admin user may already exist' ;
        airflow webserver --port 8080 &
        airflow scheduler
      "
    networks:
      - monitoring
    restart: unless-stopped

  # Airflow TEST/STAGING (for staging branch)
  airflow-test:
    image: apache/airflow:2.9.3
    container_name: airflow-test
    ports:
      - "8083:8080"
    mem_limit: 1.5g
    mem_reservation: 512m
    cpus: 0.75
    environment:
      - AIRFLOW_HOME=/opt/airflow
      - AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
      # Use FAB auth manager so 'airflow users create' controls login
      - AIRFLOW__CORE__AUTH_MANAGER=airflow.auth.managers.fab.fab_auth_manager.FabAuthManager
      - AIRFLOW__WEBSERVER__RATELIMIT_STORAGE_URI=redis://redis:6379/2
      # Optimize for resource usage - use single worker to reduce memory
      - AIRFLOW__WEBSERVER__WORKERS=1
      - AIRFLOW__WEBSERVER__WORKER_REFRESH_INTERVAL=60
      - AIRFLOW__SCHEDULER__JOB_HEARTBEAT_SEC=10
      - AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC=10
      - AIRFLOW__WEBSERVER__WEB_SERVER_MASTER_TIMEOUT=180
      - AIRFLOW__WEBSERVER__WEB_SERVER_WORKER_TIMEOUT=120
      # Make host project package and custom operators available inside the container
      - PYTHONPATH=/opt/airflow/src:/opt/airflow/operators
      # Set environment for config.py (since git commands may not work in container)
      - AIRFLOW_ENV=staging
      - GIT_BRANCH=${GIT_BRANCH:-staging}
      # PostgreSQL connection (use container name, not localhost)
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_USER=test.tradingAgent
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-tradingAgent}
      - POSTGRES_DB=postgres
    volumes:
      - ../.airflow/dags:/opt/airflow/dags
      - ../.airflow/logs-test:/opt/airflow/logs
      - ../.airflow/plugins:/opt/airflow/plugins
      # Mount custom operators package
      - ../.airflow/operators:/opt/airflow/operators
      # Mount project source (contains the trading_agent package) into the container
      - ../../src:/opt/airflow/src
      # Mount requirements.txt so we can install dependencies
      - ../../requirements.txt:/opt/airflow/requirements.txt:ro
      - airflow-db-test:/opt/airflow
    command: >
      bash -c "
        # Install critical project dependencies needed for DAGs (run in background, don't block)
        python3 -m pip install --quiet tqdm pandas psycopg2-binary requests python-dotenv > /tmp/pip_install.log 2>&1 &
        # Start Airflow services immediately (dependencies will install in background)
        airflow db migrate &&
        airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password 2014 || echo 'Admin user may already exist' ;
        airflow webserver --port 8080 &
        airflow scheduler
      "
    networks:
      - monitoring
    restart: unless-stopped

  # Airflow PROD (for main branch)
  airflow-prod:
    image: apache/airflow:2.9.3
    container_name: airflow-prod
    ports:
      - "8084:8080"
    mem_limit: 1.5g
    mem_reservation: 512m
    cpus: 0.75
    environment:
      - AIRFLOW_HOME=/opt/airflow
      - AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
      # Use FAB auth manager so 'airflow users create' controls login
      - AIRFLOW__CORE__AUTH_MANAGER=airflow.auth.managers.fab.fab_auth_manager.FabAuthManager
      - AIRFLOW__WEBSERVER__RATELIMIT_STORAGE_URI=redis://redis:6379/3
      # Optimize for resource usage - use single worker to reduce memory
      - AIRFLOW__WEBSERVER__WORKERS=1
      - AIRFLOW__WEBSERVER__WORKER_REFRESH_INTERVAL=60
      - AIRFLOW__SCHEDULER__JOB_HEARTBEAT_SEC=10
      - AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC=10
      - AIRFLOW__WEBSERVER__WEB_SERVER_MASTER_TIMEOUT=180
      - AIRFLOW__WEBSERVER__WEB_SERVER_WORKER_TIMEOUT=120
      # Make host project package and custom operators available inside the container
      - PYTHONPATH=/opt/airflow/src:/opt/airflow/operators
      # Set environment for config.py (since git commands may not work in container)
      - AIRFLOW_ENV=prod
      - GIT_BRANCH=${GIT_BRANCH:-main}
      # PostgreSQL connection (use container name, not localhost)
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_USER=prod.tradingAgent
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-tradingAgent}
      - POSTGRES_DB=postgres
    volumes:
      - ../.airflow/dags:/opt/airflow/dags
      - ../.airflow/logs-prod:/opt/airflow/logs
      - ../.airflow/plugins:/opt/airflow/plugins
      # Mount custom operators package
      - ../.airflow/operators:/opt/airflow/operators
      # Mount project source (contains the trading_agent package) into the container
      - ../../src:/opt/airflow/src
      # Mount requirements.txt so we can install dependencies
      - ../../requirements.txt:/opt/airflow/requirements.txt:ro
      - airflow-db-prod:/opt/airflow
    command: >
      bash -c "
        # Install critical project dependencies needed for DAGs (run in background, don't block)
        python3 -m pip install --quiet tqdm pandas psycopg2-binary requests python-dotenv > /tmp/pip_install.log 2>&1 &
        # Start Airflow services immediately (dependencies will install in background)
        airflow db migrate &&
        airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password 2014 || echo 'Admin user may already exist' ;
        airflow webserver --port 8080 &
        airflow scheduler
      "
    networks:
      - monitoring
    restart: unless-stopped

  # PostgreSQL for macro databases (fred, bis, bls, eurostat, imf, etc.)
  postgres:
    image: postgres:15
    container_name: postgres
    ports:
      - "55432:5432"
    environment:
      - POSTGRES_USER=tradingAgent
      # Use host env POSTGRES_PASSWORD if set, otherwise default
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-tradingAgent}
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - monitoring
    restart: unless-stopped

  # Redis for Airflow rate limiting backend
  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6379:6379"
    networks:
      - monitoring
    restart: unless-stopped

  # Jenkins CI/CD server
  jenkins:
    image: jenkins-custom:lts  # Custom image with kubectl, kind, and Docker CLI pre-installed
    container_name: jenkins
    ports:
      - "8081:8080"  # Match current port forwarding setup (8081 on host)
      - "50000:50000"
    environment:
      - JAVA_OPTS=-Djenkins.install.runSetupWizard=false
      # JIRA configuration for feature branch validation
      - JIRA_URL=https://vittorioapi91.atlassian.net
      - JIRA_USER=vittorioapi91@gmail.com
      - JIRA_API_TOKEN=ATATT3xFfGF0X2ORyNBVPCGC44Z2a0VdyOa2IpPHQ-Hoe3ER0z7J19uGZisnrDMgJNjnWVfgUD1TBnJQpLxhe3mmPU4RXoQrZYPzJklEsesER06GUaSHq0SJcVXcIzXzRkwbA8j31_8paZCn0H_PB06naDKgdiPP6Wcl1FJppzvtRDRfZKU-Bew=2190D79E
    volumes:
      - ../.jenkins/data:/var/jenkins_home  # Mount extracted Jenkins data
      - /var/run/docker.sock:/var/run/docker.sock  # Docker socket for docker build
      - ${HOME}/.kube:/root/.kube  # Kubernetes config for kubectl access
    networks:
      - monitoring
    restart: unless-stopped
    user: root  # Needed for Docker socket access
    entrypoint: >
      bash -c "
        # Update kubeconfig to use host.docker.internal for localhost connections (Mac Docker Desktop)
        # Tools (docker, kubectl, kind) are already pre-installed in the custom image
        if [ -f /root/.kube/config ] && command -v kubectl &> /dev/null; then
          # Backup original config
          cp /root/.kube/config /root/.kube/config.bak
          # Replace 127.0.0.1 and localhost with host.docker.internal
          # Use perl for better regex support (escape backslashes properly)
          perl -pi -e 's|https://127\.0\.0\.1:(\d+)|https://host.docker.internal:$1|g' /root/.kube/config || true
          perl -pi -e 's|https://localhost:(\d+)|https://host.docker.internal:$1|g' /root/.kube/config || true
          # Add insecure-skip-tls-verify for kind-trading-cluster (cert doesn't include host.docker.internal)
          # This is safe for local development only
          kubectl config set-cluster kind-trading-cluster --insecure-skip-tls-verify=true 2>/dev/null || true
        fi
        # Start Jenkins with original entrypoint
        exec /usr/local/bin/jenkins.sh
      "

volumes:
  prometheus-data:
  grafana-data:
  mlflow-data:
  postgres-data:
  airflow-db-dev:
  airflow-db-test:
  airflow-db-prod:
  redis-data:

networks:
  monitoring:
    driver: bridge

