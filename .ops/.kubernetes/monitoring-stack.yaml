apiVersion: v1
kind: Namespace
metadata:
  name: trading-monitoring
---
# Prometheus ConfigMap (scrape config similar to Docker setup)
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: trading-monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
      external_labels:
        cluster: 'macro-cycle-hmm'
        environment: 'development'

    scrape_configs:
      - job_name: 'prometheus'
        static_configs:
          - targets: ['localhost:9090']

      - job_name: 'macro-cycle-hmm'
        static_configs:
          - targets: ['hmm-metrics:8000']
            labels:
              service: 'hmm-model'
              component: 'model'

      - job_name: 'mlflow'
        static_configs:
          - targets: ['mlflow:5000']
            labels:
              service: 'mlflow'
              component: 'tracking'
---
# Prometheus Deployment & Service
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  namespace: trading-monitoring
  labels:
    app: prometheus
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      containers:
        - name: prometheus
          image: prom/prometheus:latest
          args:
            - "--config.file=/etc/prometheus/prometheus.yml"
            - "--storage.tsdb.path=/prometheus"
            - "--storage.tsdb.retention.time=30d"
            - "--web.enable-lifecycle"
          ports:
            - containerPort: 9090
          resources:
            requests:
              cpu: "100m"
              memory: "256Mi"
            limits:
              cpu: "500m"
              memory: "1Gi"
          volumeMounts:
            - name: prometheus-config
              mountPath: /etc/prometheus
            - name: prometheus-storage
              mountPath: /prometheus
      volumes:
        - name: prometheus-config
          configMap:
            name: prometheus-config
        - name: prometheus-storage
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  namespace: trading-monitoring
  labels:
    app: prometheus
spec:
  type: LoadBalancer
  ports:
    - port: 9090
      targetPort: 9090
      protocol: TCP
      name: http
  selector:
    app: prometheus
---
# Grafana Deployment & Service
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: trading-monitoring
  labels:
    app: grafana
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      containers:
        - name: grafana
          image: grafana/grafana:latest
          env:
            - name: GF_SECURITY_ADMIN_USER
              value: "admin"
            - name: GF_SECURITY_ADMIN_PASSWORD
              value: "admin"
            - name: GF_USERS_ALLOW_SIGN_UP
              value: "false"
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "512Mi"
          ports:
            - containerPort: 3000
          volumeMounts:
            - name: grafana-storage
              mountPath: /var/lib/grafana
      volumes:
        - name: grafana-storage
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: trading-monitoring
  labels:
    app: grafana
spec:
  type: LoadBalancer
  ports:
    - port: 3000
      targetPort: 3000
      protocol: TCP
      name: http
  selector:
    app: grafana
---
# MLflow Deployment & Service
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mlflow
  namespace: trading-monitoring
  labels:
    app: mlflow
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mlflow
  template:
    metadata:
      labels:
        app: mlflow
    spec:
      containers:
        - name: mlflow
          image: ghcr.io/mlflow/mlflow:v2.8.1
          env:
            - name: MLFLOW_BACKEND_STORE_URI
              value: "sqlite:///mlflow.db"
            - name: MLFLOW_DEFAULT_ARTIFACT_ROOT
              value: "/mlflow/artifacts"
          resources:
            requests:
              cpu: "100m"
              memory: "256Mi"
            limits:
              cpu: "500m"
              memory: "1Gi"
          ports:
            - containerPort: 5000
          volumeMounts:
            - name: mlflow-storage
              mountPath: /mlflow
          command: [ "mlflow", "server", "--host", "0.0.0.0", "--port", "5000" ]
      volumes:
        - name: mlflow-storage
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: mlflow
  namespace: trading-monitoring
  labels:
    app: mlflow
spec:
  type: LoadBalancer
  ports:
    - port: 5000
      targetPort: 5000
      protocol: TCP
      name: http
  selector:
    app: mlflow
---
# Airflow Deployment & Service (web + scheduler in one pod)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow
  namespace: trading-monitoring
  labels:
    app: airflow
spec:
  replicas: 1
  selector:
    matchLabels:
      app: airflow
  template:
    metadata:
      labels:
        app: airflow
    spec:
      containers:
        - name: airflow
          image: apache/airflow:2.9.3
          env:
            - name: AIRFLOW_HOME
              value: /opt/airflow
            - name: AIRFLOW__CORE__DAGS_FOLDER
              value: /opt/airflow/dags
            - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
              value: sqlite:////opt/airflow/airflow.db
          resources:
            requests:
              cpu: "200m"
              memory: "512Mi"
            limits:
              cpu: "1000m"
              memory: "2Gi"
          ports:
            - containerPort: 8080
          volumeMounts:
            - name: airflow-dags
              mountPath: /opt/airflow/dags
            - name: airflow-logs
              mountPath: /opt/airflow/logs
          command:
            - bash
            - -c
            - |
              airflow db migrate
              airflow standalone
      volumes:
        - name: airflow-dags
          emptyDir: {}
        - name: airflow-logs
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: airflow
  namespace: trading-monitoring
  labels:
    app: airflow
spec:
  type: LoadBalancer
  ports:
    - port: 8080
      targetPort: 8080
      protocol: TCP
      name: http
  selector:
    app: airflow
---
# PostgreSQL Deployment & Service
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres
  namespace: trading-monitoring
  labels:
    app: postgres
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
        - name: postgres
          image: postgres:15
          env:
            - name: POSTGRES_USER
              value: tradingAgent
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: password
          resources:
            requests:
              cpu: "100m"
              memory: "256Mi"
            limits:
              cpu: "500m"
              memory: "1Gi"
          ports:
            - containerPort: 5432
          volumeMounts:
            - name: postgres-data
              mountPath: /var/lib/postgresql/data
      volumes:
        - name: postgres-data
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: postgres
  namespace: trading-monitoring
  labels:
    app: postgres
spec:
  type: ClusterIP
  ports:
    - port: 5432
      targetPort: 5432
      protocol: TCP
      name: postgres
  selector:
    app: postgres
---
# Secret for PostgreSQL password (set once per cluster)
apiVersion: v1
kind: Secret
metadata:
  name: postgres-secret
  namespace: trading-monitoring
type: Opaque
stringData:
  password: tradingAgent

---
# Feast helper Deployment
#
# Feast is primarily a CLI / SDK, not a long-running HTTP service.
# This Deployment gives you a running pod visible in the Kubernetes
# Dashboard where you can exec into the container and run Feast
# commands against your feature repo (for example, using kubectl cp
# to copy the repo into the pod, or by building a custom image).
apiVersion: apps/v1
kind: Deployment
metadata:
  name: feast
  namespace: trading-monitoring
  labels:
    app: feast
spec:
  replicas: 1
  selector:
    matchLabels:
      app: feast
  template:
    metadata:
      labels:
        app: feast
    spec:
      containers:
        - name: feast
          image: python:3.11-slim
          command: ["sleep", "infinity"]
          env:
            - name: FEAST_REPO_PATH
              value: "/workspace/feast_repo"
          # You can exec into this pod from the Dashboard or kubectl:
          #   kubectl exec -it -n trading-monitoring deploy/feast -- bash
          # and then pip install feast / interact with your repo.
          resources:
            requests:
              cpu: "50m"
              memory: "64Mi"
            limits:
              cpu: "200m"
              memory: "256Mi"



